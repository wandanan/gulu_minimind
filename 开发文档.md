好的，遵命。这份文档是为AI辅助开发工作流（如Aide）设计的完整、独立、自包含的开发指南。其中包含了所有必要的背景信息、项目结构、完整代码和操作指令，没有任何对先前对话的引用。

项目文档：咕噜LTC虚拟生命训练框架 V1.0
1. 项目概述

本项目旨在指导开发者利用minimind训练框架，从零开始，分阶段训练一个基于**液态时间常数网络（Liquid Time-Constant Networks, LTC）**的虚拟生命智能体——“咕噜”。

最终目标：产出一个具备三个层次能力的AI模型：

基础本能：通过监督学习获得。

目标导向决策：通过强化学习微调获得。

个性化学习潜力：为未来的在线学习阶段打下基础。

核心技术栈:

AI模型: Liquid Time-Constant Networks (LTC)

深度学习框架: PyTorch

训练框架: MiniMind (作为结构参考)

强化学习库: Stable-Baselines3, Gymnasium

2. 项目设置与最终结构

在开始编码前，请确保你的minimind项目根目录下有以下结构。所有新文件都将依据此结构创建。

code
Code
download
content_copy
expand_less

D:\SMALL_APP\MINIMIND\
│  .gitignore
│  README.md
│  requirements.txt
│  gollum_data_generator.py   <-- [新增]
│  eval_gollum_sl.py            <-- [新增]
│
├─dataset
│  │  gollum_dataset.py          <-- [新增]
│  │  __init__.py
│  │
│  └─gollum_data                  <-- [新增, 自动生成]
│      ├─train
│      └─val
│
├─gym_env                       <-- [新增]
│  │  gollum_env.py              <-- [新增]
│  └─__init__.py
│
├─model
│  │  GollumLTCConfig.py         <-- [新增]
│  │  model_gollum_ltc.py        <-- [新增]
│  └─__init__.py
│
└─trainer
   │  train_gollum_sl.py         <-- [新增]
   │  train_gollum_rl.py         <-- [新增]
   └─... (minimind原有文件)
3. 阶段零：生成“本能”数据集

目标：创建一个合成数据集，该数据集由一个基于规则的“老师模型”生成。每一条数据都记录了咕噜在接收到一系列环境刺激后，其内在状态（能量、安全感、信任度）随时间演化的过程。

操作流程：

在项目根目录 D:\Small_APP\minimind 下创建文件 gollum_data_generator.py。

将下面的完整代码粘贴到文件中。

打开终端，导航到项目根目录，运行命令：python gollum_data_generator.py。

脚本执行完毕后，dataset/gollum_data/ 目录下将自动创建 train 和 val 两个子目录，并填充满 .npy 格式的数据文件。

文件代码: gollum_data_generator.py
code
Python
download
content_copy
expand_less
IGNORE_WHEN_COPYING_START
IGNORE_WHEN_COPYING_END
import numpy as np
import os
import uuid

class GollumRuleModel:
    """
    一个基于规则和ODE思想的“老师模型”，用于生成训练数据。
    """
    def __init__(self):
        # 内部状态: [能量, 安全感, 信任度]
        self.state = np.array([0.8, 0.7, 0.5], dtype=np.float32)
        # 时间常数 (Tau): 状态变化的惯性，值越大变化越慢
        self.tau = np.array([2.0, 5.0, 10.0], dtype=np.float32) # 能量变化快, 安全感中等, 信任度变化慢
        self.dt = 0.1 # 时间步长

    def reset(self):
        self.state = np.array([0.8, 0.7, 0.5], dtype=np.float32)

    def step(self, env_input):
        # 环境输入: [玩具(0/1), 危险(0/1), 食物(0/1)]
        toy, danger, food = env_input

        # 1. 计算目标状态 (Target State)
        target_state = self.state.copy()
        
        # 能量动态
        energy_decay = -0.05 # 基础能量消耗
        energy_gain = 0.8 * food
        target_state[0] += energy_decay + energy_gain

        # 安全感动态 (受危险和玩具影响)
        safety_target = 0.5 - 0.9 * danger + 0.1 * toy * self.state[2] # 玩具带来的安全感依赖于信任度
        target_state[1] = safety_target
        
        # 信任度动态 (只有在安全且有玩具互动时才缓慢增加)
        trust_gain = 0.02 * toy * (self.state[1] - 0.5) # 只有在感到安全(>0.5)时才会增加信任
        target_state[2] += trust_gain

        # 2. 使用一阶动态系统公式更新状态
        # d(state)/dt = (target - state) / tau
        d_state = (target_state - self.state) / self.tau
        self.state += d_state * self.dt
        
        # 3. 限制状态值在 [0, 1] 范围内
        self.state = np.clip(self.state, 0, 1)
        
        return self.state.copy()

def generate_and_save_data(num_samples, save_dir, seq_len=100):
    """
    生成并保存数据样本。
    """
    model = GollumRuleModel()
    
    for i in range(num_samples):
        model.reset()
        
        # 随机生成一个环境刺激序列
        # 模式: 20%时间有食物, 10%时间有危险, 30%时间有玩具
        inputs = np.zeros((seq_len, 3), dtype=np.float32)
        inputs[np.random.rand(seq_len) < 0.2, 2] = 1 # 食物
        inputs[np.random.rand(seq_len) < 0.1, 1] = 1 # 危险
        inputs[np.random.rand(seq_len) < 0.3, 0] = 1 # 玩具
        # 确保事件不总是同时发生
        inputs[inputs.sum(axis=1) > 1] = 0
        inputs[0,:] = 0 # 初始状态为平静

        outputs = np.zeros((seq_len, 3), dtype=np.float32)
        
        for t in range(seq_len):
            outputs[t] = model.step(inputs[t])
            
        data_sample = {
            'input': inputs,
            'output': outputs
        }
        
        # 使用uuid确保文件名唯一
        filename = os.path.join(save_dir, f"sample_{uuid.uuid4()}.npy")
        np.save(filename, data_sample)
        
    print(f"Generated {num_samples} samples in '{save_dir}'")

if __name__ == "__main__":
    print("Starting dataset generation for Gollum...")
    
    # 确保文件夹存在
    train_dir = "./dataset/gollum_data/train"
    val_dir = "./dataset/gollum_data/val"
    if not os.path.exists(train_dir):
        os.makedirs(train_dir)
    if not os.path.exists(val_dir):
        os.makedirs(val_dir)

    # 生成数据: 80% 训练, 20% 验证
    generate_and_save_data(2000, train_dir)
    generate_and_save_data(400, val_dir)
    
    print("Gollum dataset generation complete!")
4. 阶段一：监督学习 (SL) - 塑造“本能核心”

目标：训练一个LTC网络，使其能够输入环境刺激序列，并输出与“老师模型”尽可能一致的内在状态演化序列。

4.1 代码实现

操作流程：

创建文件 model/GollumLTCConfig.py

创建文件 model/model_gollum_ltc.py (注意：这里提供了LTC核心逻辑的简化但有效的实现)

创建文件 dataset/gollum_dataset.py

创建文件 trainer/train_gollum_sl.py

创建文件 eval_gollum_sl.py

文件代码: model/GollumLTCConfig.py
code
Python
download
content_copy
expand_less
IGNORE_WHEN_COPYING_START
IGNORE_WHEN_COPYING_END
from dataclasses import dataclass

@dataclass
class GollumLTCConfig:
    input_size: int = 3    # 环境输入维度: [玩具, 危险, 食物]
    hidden_size: int = 32  # 隐藏层大小/神经元数量
    output_size: int = 3   # 内部状态输出维度: [能量, 安全感, 信任度]
    time_step: float = 0.1 # ODE求解器的时间步长，应与数据生成器一致
文件代码: model/model_gollum_ltc.py
code
Python
download
content_copy
expand_less
IGNORE_WHEN_COPYING_START
IGNORE_WHEN_COPYING_END
import torch
import torch.nn as nn
from .GollumLTCConfig import GollumLTCConfig

class LTCCell(nn.Module):
    """
    一个简化的液态时间常数单元。
    这个实现捕捉了LTC的核心思想：动态的时间常数(tau)和状态依赖的内部动态。
    """
    def __init__(self, input_size, hidden_size):
        super(LTCCell, self).__init__()
        self.hidden_size = hidden_size
        
        # 线性层将输入和前一状态映射到内部表示
        self.w_ih = nn.Linear(input_size, hidden_size)
        self.w_hh = nn.Linear(hidden_size, hidden_size)
        
        # 关键: 计算动态时间常数 1/tau 的层
        # 它接收输入和隐藏状态，输出每个神经元的 1/tau
        self.w_tau = nn.Linear(input_size + hidden_size, hidden_size)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x, hx, time_step):
        # x: [batch_size, input_size]
        # hx: [batch_size, hidden_size] (前一时刻的隐藏状态)
        
        # 计算内部互联和输入影响
        pre_activation = self.w_ih(x) + self.w_hh(hx)
        
        # 计算动态时间常数 1/tau
        # 将输入和隐藏状态拼接
        tau_input = torch.cat((x, hx), dim=1)
        inv_tau = self.sigmoid(self.w_tau(tau_input)) # 1/tau，值域(0, 1)
        
        # 使用Forward Euler方法求解ODE
        # dx/dt = -x/tau + f(I,x)  =>  dx/dt = -x * (1/tau) + activation(pre_activation)
        # x_new = x_old + dt * dx/dt
        
        activation = torch.tanh(pre_activation)
        d_h = -hx * inv_tau + activation
        h_new = hx + time_step * d_h
        
        return h_new

class GollumLTC(nn.Module):
    def __init__(self, config: GollumLTCConfig):
        super(GollumLTC, self).__init__()
        self.config = config
        self.ltc_cell = LTCCell(config.input_size, config.hidden_size)
        self.output_mapping = nn.Linear(config.hidden_size, config.output_size)
        self.final_activation = nn.Sigmoid() # 确保输出在[0, 1]

    def forward(self, x, h0=None):
        # x shape: (batch_size, sequence_length, input_size)
        batch_size, seq_len, _ = x.shape
        
        if h0 is None:
            # 初始化隐藏状态为0
            h0 = torch.zeros(batch_size, self.config.hidden_size, device=x.device)
            
        outputs = []
        ht = h0
        
        # 沿时间序列展开
        for t in range(seq_len):
            xt = x[:, t, :]
            ht = self.ltc_cell(xt, ht, self.config.time_step)
            outputs.append(ht)
            
        # 堆叠所有时间步的隐藏状态
        hidden_states = torch.stack(outputs, dim=1) # [batch, seq, hidden]
        
        # 映射到最终输出
        output = self.output_mapping(hidden_states)
        return self.final_activation(output)
文件代码: dataset/gollum_dataset.py
code
Python
download
content_copy
expand_less
IGNORE_WHEN_COPYING_START
IGNORE_WHEN_COPYING_END
import torch
from torch.utils.data import Dataset
import numpy as np
import os
import glob

class GollumDataset(Dataset):
    def __init__(self, data_dir):
        super().__init__()
        self.file_paths = glob.glob(os.path.join(data_dir, "*.npy"))
        if not self.file_paths:
            raise FileNotFoundError(f"No .npy files found in directory: {data_dir}. Did you run the data generator script?")

    def __len__(self):
        return len(self.file_paths)

    def __getitem__(self, idx):
        data = np.load(self.file_paths[idx], allow_pickle=True).item()
        
        # [seq_len, num_features]
        input_sequence = torch.from_numpy(data['input']).float()
        target_sequence = torch.from_numpy(data['output']).float()
        
        return input_sequence, target_sequence
文件代码: trainer/train_gollum_sl.py
code
Python
download
content_copy
expand_less
IGNORE_WHEN_COPYING_START
IGNORE_WHEN_COPYING_END
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm
import os

from model.GollumLTCConfig import GollumLTCConfig
from model.model_gollum_ltc import GollumLTC
from dataset.gollum_dataset import GollumDataset

# --- 1. 配置 ---
print("--- Initializing Gollum SL Training ---")
config = GollumLTCConfig()
device = "cuda" if torch.cuda.is_available() else "cpu"
epochs = 25
batch_size = 64
learning_rate = 0.005
output_dir = "./output"
save_path = os.path.join(output_dir, "gollum_instinct_core.pth")

print(f"Configuration: device={device}, epochs={epochs}, batch_size={batch_size}, lr={learning_rate}")

# --- 2. 数据加载 ---
print("--- Loading Datasets ---")
train_dataset = GollumDataset("./dataset/gollum_data/train")
val_dataset = GollumDataset("./dataset/gollum_data/val")
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)
val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4)
print(f"Found {len(train_dataset)} training samples and {len(val_dataset)} validation samples.")

# --- 3. 模型、损失函数、优化器 ---
print("--- Setting up Model, Loss, and Optimizer ---")
model = GollumLTC(config).to(device)
loss_fn = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)

# --- 4. 训练循环 ---
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

best_val_loss = float('inf')
print("--- Starting Training Loop ---")
for epoch in range(epochs):
    # 训练阶段
    model.train()
    total_train_loss = 0
    progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} [Train]", leave=False)
    for inputs, targets in progress_bar:
        inputs, targets = inputs.to(device), targets.to(device)
        
        optimizer.zero_grad()
        predictions = model(inputs)
        loss = loss_fn(predictions, targets)
        loss.backward() # PyTorch自动处理BPTT
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # 梯度裁剪
        optimizer.step()
        
        total_train_loss += loss.item()
        progress_bar.set_postfix(loss=loss.item())
    
    avg_train_loss = total_train_loss / len(train_loader)

    # 验证阶段
    model.eval()
    total_val_loss = 0
    progress_bar_val = tqdm(val_loader, desc=f"Epoch {epoch+1}/{epochs} [Val]", leave=False)
    with torch.no_grad():
        for inputs, targets in progress_bar_val:
            inputs, targets = inputs.to(device), targets.to(device)
            predictions = model(inputs)
            loss = loss_fn(predictions, targets)
            total_val_loss += loss.item()
            progress_bar_val.set_postfix(loss=loss.item())
    
    avg_val_loss = total_val_loss / len(val_loader)
    
    scheduler.step()
    
    print(f"Epoch {epoch+1}/{epochs} -> Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}, LR: {scheduler.get_last_lr()[0]:.6f}")

    # 保存最佳模型
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        torch.save(model.state_dict(), save_path)
        print(f"  -> New best model saved to {save_path} with Val Loss: {best_val_loss:.6f}")

print("\n--- Training Complete ---")
print(f"Best model saved to {save_path}")
文件代码: eval_gollum_sl.py
code
Python
download
content_copy
expand_less
IGNORE_WHEN_COPYING_START
IGNORE_WHEN_COPYING_END
import torch
import numpy as np
import matplotlib.pyplot as plt
from dataset.gollum_dataset import GollumDataset
from model.GollumLTCConfig import GollumLTCConfig
from model.model_gollum_ltc import GollumLTC

def plot_comparison(model, dataset, sample_idx=0):
    # 加载一个样本
    input_seq, target_seq = dataset[sample_idx]
    
    # 准备模型输入
    model.eval()
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)
    input_tensor = input_seq.unsqueeze(0).to(device) # 增加batch维度

    # 获取模型预测
    with torch.no_grad():
        prediction_tensor = model(input_tensor)
    
    # 将数据转为numpy用于绘图
    target_np = target_seq.numpy()
    prediction_np = prediction_tensor.squeeze(0).cpu().numpy()
    
    # 绘图
    labels = ['Energy', 'Safety', 'Trust']
    colors = ['green', 'blue', 'red']
    
    fig, axes = plt.subplots(3, 1, figsize=(12, 8), sharex=True)
    fig.suptitle(f'Gollum State Dynamics - Sample {sample_idx}', fontsize=16)
    
    for i in range(3):
        axes[i].plot(target_np[:, i], label=f'Ground Truth ({labels[i]})', color=colors[i], linestyle='--')
        axes[i].plot(prediction_np[:, i], label=f'LTC Prediction ({labels[i]})', color=colors[i])
        axes[i].set_ylabel('State Value')
        axes[i].legend()
        axes[i].grid(True, linestyle=':', alpha=0.6)
        axes[i].set_ylim(0, 1)
        
    axes[2].set_xlabel('Time Steps')
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()

if __name__ == '__main__':
    # 加载配置和模型
    config = GollumLTCConfig()
    model = GollumLTC(config)
    
    # 加载训练好的权重
    model_path = "./output/gollum_instinct_core.pth"
    try:
        model.load_state_dict(torch.load(model_path))
        print(f"Model weights loaded from {model_path}")
    except FileNotFoundError:
        print(f"Error: Model file not found at {model_path}. Please run training first.")
        exit()

    # 加载验证集进行评估
    val_dataset = GollumDataset("./dataset/gollum_data/val")
    
    # 绘制指定样本的对比图
    plot_comparison(model, val_dataset, sample_idx=10)
4.2 操作流程与评估

执行训练：

code
Bash
download
content_copy
expand_less
IGNORE_WHEN_COPYING_START
IGNORE_WHEN_COPYING_END
python trainer/train_gollum_sl.py

定量评估：观察训练过程中的 Val Loss。如果它稳步下降并稳定在一个较低的值，说明模型正在有效学习。

定性评估：训练结束后，运行评估脚本：

code
Bash
download
content_copy
expand_less
IGNORE_WHEN_COPYING_START
IGNORE_WHEN_COPYING_END
python eval_gollum_sl.py

脚本会弹出一张图，其中虚线代表“老师”的真实状态变化，实线代表LTC“学生”的预测。如果两条线高度重合，恭喜你，咕噜的“本能核心”已经塑造成功！

5. 阶段二：强化学习 (RL) - 孕育“目标导向”

目标：在已具备“本能”的咕噜模型基础上，通过强化学习，让它学会主动选择动作以最大化自身的长期“福祉”（例如，保持高能量和高安全感）。

5.1 代码实现

操作流程：

确保你已安装 stable-baselines3 和 gymnasium。

code
Bash
download
content_copy
expand_less
IGNORE_WHEN_COPYING_START
IGNORE_WHEN_COPYING_END
pip install stable-baselines3[extra] gymnasium

创建 gym_env 文件夹和 __init__.py 文件。

创建文件 gym_env/gollum_env.py。

创建文件 trainer/train_gollum_rl.py。

文件代码: gym_env/gollum_env.py
code
Python
download
content_copy
expand_less
IGNORE_WHEN_COPYING_START
IGNORE_WHEN_COPYING_END
import gymnasium as gym
from gymnasium import spaces
import numpy as np

class GollumEnv(gym.Env):
    """
    一个简化的Gym环境，咕噜可以在其中生存和学习。
    """
    metadata = {'render_modes': ['human']}

    def __init__(self):
        super(GollumEnv, self).__init__()
        
        # 动作空间: [0:休息, 1:探索(可能发现食物), 2:躲藏(增加安全感)]
        self.action_space = spaces.Discrete(3)
        
        # 观察空间: [能量, 安全感, 信任度(对环境)]
        self.observation_space = spaces.Box(low=0, high=1, shape=(3,), dtype=np.float32)
        
        self._max_steps = 200

    def _get_obs(self):
        return self._internal_state

    def _get_info(self):
        return {"steps": self._current_step}

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        
        # 初始化内在状态
        self._internal_state = np.array([0.8, 0.7, 0.5], dtype=np.float32)
        self._current_step = 0
        
        return self._get_obs(), self._get_info()

    def step(self, action):
        # 基础消耗
        self._internal_state[0] -= 0.01  # 能量自然衰减
        self._internal_state[1] -= 0.005 # 安全感自然衰减

        # 根据动作更新状态
        if action == 0: # 休息
            self._internal_state[0] += 0.005 # 缓慢恢复能量
        elif action == 1: # 探索
            self._internal_state[0] -= 0.02 # 探索消耗更多能量
            self._internal_state[1] -= 0.02 # 探索降低安全感
            if self.np_random.random() < 0.2: # 20%概率找到食物
                self._internal_state[0] += 0.5
        elif action == 2: # 躲藏
            self._internal_state[1] += 0.1 # 显著增加安全感

        # 限制状态值在 [0, 1]
        self._internal_state = np.clip(self._internal_state, 0, 1)

        # 计算奖励
        # 核心奖励: 保持高能量和高安全感
        reward = self._internal_state[0] + self._internal_state[1]
        # 惩罚: 能量或安全感过低
        if self._internal_state[0] < 0.2 or self._internal_state[1] < 0.2:
            reward -= 1.0

        # 判断是否结束
        self._current_step += 1
        terminated = bool(self._internal_state[0] <= 0.01) # 能量耗尽则死亡
        truncated = bool(self._current_step >= self._max_steps) # 达到最大步数
        
        return self._get_obs(), reward, terminated, truncated, self._get_info()
文件代码: trainer/train_gollum_rl.py
code
Python
download
content_copy
expand_less
IGNORE_WHEN_COPYING_START
IGNORE_WHEN_COPYING_END
import torch
import torch.nn as nn
from stable_baselines3 import PPO
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.policies import ActorCriticPolicy
import gymnasium as gym
import os

from model.GollumLTCConfig import GollumLTCConfig
from model.model_gollum_ltc import GollumLTC
from gym_env.gollum_env import GollumEnv

# 1. 自定义特征提取器，将我们的LTC模型作为RL策略的“大脑”
# 注意: PPO通常处理单步观察，但LTC是时序模型。
# 这里我们做一个简化：将LTC作为一个非线性特征提取器，每次只处理一步。
# 这保留了LTC的动态特性，但没有利用其长时记忆。更复杂的实现会使用Recurrent PPO。
class LTCAsFeatureExtractor(BaseFeaturesExtractor):
    def __init__(self, observation_space: gym.spaces.Box, features_dim: int):
        # features_dim 是 PPO Actor-Critic 网络的输入维度
        super().__init__(observation_space, features_dim)
        
        # 我们让LTC的隐藏层大小与PPO期望的特征维度一致
        config = GollumLTCConfig(input_size=observation_space.shape[0], hidden_size=features_dim, output_size=features_dim)
        
        # 注意：这里我们只使用LTC的cell作为特征提取器，而不是完整的GollumLTC模型
        # 因为我们不需要它的output_mapping层
        self.ltc_cell = GollumLTC(config).ltc_cell
        
        # 加载阶段一的预训练权重！这是关键！
        pretrain_path = "./output/gollum_instinct_core.pth"
        try:
            full_model_state_dict = torch.load(pretrain_path)
            # 只加载ltc_cell部分的权重
            cell_state_dict = {k.replace('ltc_cell.', ''): v for k, v in full_model_state_dict.items() if 'ltc_cell' in k}
            self.ltc_cell.load_state_dict(cell_state_dict)
            print("Successfully loaded pre-trained weights into LTC cell for RL.")
        except Exception as e:
            print(f"Warning: Could not load pre-trained weights. Training from scratch. Error: {e}")
            
        # RL需要一个隐藏状态来进行循环计算
        self.hidden_state = None

    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        # PPO在每个环境中并行运行，我们需要为每个环境维护一个隐藏状态
        batch_size = observations.shape[0]
        device = observations.device
        
        if self.hidden_state is None or self.hidden_state.shape[0] != batch_size:
            self.hidden_state = torch.zeros(batch_size, self.ltc_cell.hidden_size, device=device)
        
        # 分离隐藏状态，防止梯度流回上一步
        self.hidden_state = self.hidden_state.detach()
        
        # LTC cell前向传播一步，更新隐藏状态
        self.hidden_state = self.ltc_cell(observations, self.hidden_state, time_step=0.1)
        
        return self.hidden_state

class GollumPolicy(ActorCriticPolicy):
    def __init__(self, *args, **kwargs):
        super().__init__(
            *args,
            **kwargs,
            features_extractor_class=LTCAsFeatureExtractor,
            features_extractor_kwargs=dict(features_dim=64), # 定义LTC隐藏层大小
        )

# 2. 训练流程
if __name__ == '__main__':
    print("--- Initializing Gollum RL Training ---")
    env = GollumEnv()
    
    # 检查预训练模型是否存在
    if not os.path.exists("./output/gollum_instinct_core.pth"):
        print("Error: Pre-trained model not found. Please run SL training first.")
    else:
        # 使用自定义策略创建PPO模型
        model = PPO(GollumPolicy, env, verbose=1, tensorboard_log="./gollum_tensorboard/")
        
        print("--- Starting RL Training Loop ---")
        # 训练模型
        model.learn(total_timesteps=200000, progress_bar=True)
        
        # 保存最终模型
        save_path = "./output/gollum_rl_tuned"
        model.save(save_path)
        
        print("\n--- RL Training Complete ---")
        print(f"Decision-making model saved to {save_path}.zip")
5.2 操作流程

执行训练：

code
Bash
download
content_copy
expand_less
IGNORE_WHEN_COPYING_START
IGNORE_WHEN_COPYING_END
python trainer/train_gollum_rl.py

观察训练：你会在终端看到PPO算法的训练日志，包括奖励（ep_rew_mean）、损失等。一个成功的训练会看到平均回合奖励 ep_rew_mean 随时间稳定增长。

最终产出：训练结束后，你将在 output/ 目录下得到一个 gollum_rl_tuned.zip 文件。这个文件打包了完整的、能够做出决策的咕噜模型，可以直接在游戏环境中加载和使用。

这份完整的开发文档提供了所有必要步骤和代码，你可以按照顺序执行，最终完成一个具备基础本能和初步决策能力的咕噜AI。